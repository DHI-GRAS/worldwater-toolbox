{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99b7dea0-fc04-48c1-8600-f991ab40c323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray\n",
    "import openeo\n",
    "\n",
    "import openeo\n",
    "from openeo.processes import exp, array_element, log \n",
    "from openeo.extra.spectral_indices.spectral_indices import append_index\n",
    "from openeo.udf.debug import inspect\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11e52975-3d43-4271-a69f-95bfd23b3863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated using refresh token.\n"
     ]
    }
   ],
   "source": [
    "connection = openeo.connect(\n",
    "    url=\"openeo-dev.vito.be\"\n",
    "    # url=\"openeo.vito.be\"\n",
    ").authenticate_oidc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7443a2db",
   "metadata": {},
   "source": [
    "# S2 Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0480afaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date      = '2022-01-03'\n",
    "end_date        = '2022-01-08'\n",
    "spatial_extent  = {'west': -74.06810760, 'east': -73.90597343, 'south': 4.689864510, 'north': 4.724080996, 'crs': 'epsg:4326'}\n",
    "\n",
    "# Selecting B03 was needed to make sure that the angles bands have 10 m resolution\n",
    "s2_cube = connection.load_collection(\n",
    "    'SENTINEL2_L2A_SENTINELHUB',\n",
    "    spatial_extent = spatial_extent,\n",
    "    temporal_extent = [start_date, end_date],\n",
    "    bands = ['B03', 'sunAzimuthAngles', 'sunZenithAngles'] \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7be836b-58b1-4ca9-94d1-d5c023c3fffa",
   "metadata": {},
   "source": [
    "# DEM data (COPERNICUS_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c22e5ed-aacb-4cad-85d0-cff999f9cc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dem_cube = connection.load_collection(\n",
    "    \"COPERNICUS_30\",\n",
    "    spatial_extent = spatial_extent,\n",
    "    temporal_extent=[\"2010-01-01\", \"2030-12-31\"],\n",
    ")\n",
    "\n",
    "dem_cube = dem_cube.max_time()\n",
    "dem_cube = dem_cube.resample_cube_spatial(s2_cube)\n",
    "merged_cube = s2_cube.merge_cubes(dem_cube)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d498e12c-d677-4825-90b7-e47bbffa15d7",
   "metadata": {},
   "source": [
    "# Hillshade process with UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f720ff5-8687-4654-b04c-6a79da5c9470",
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_code = \"\"\"\n",
    "\n",
    "from openeo.udf import XarrayDataCube\n",
    "from openeo.udf.debug import inspect\n",
    "import numpy as np\n",
    "from hillshade.hillshade import hillshade\n",
    "\n",
    "\n",
    "def rasterize(azimuth, resolution=None):\n",
    "    # Convert the azimuth into its components on the XY-plane. Depending on the value of the\n",
    "    # azimuth either the x or the y component of the resulting vector is scaled to 1, so that\n",
    "    # it can be used conveniently to walk a grid.\n",
    "\n",
    "    azimuth = np.deg2rad(azimuth)\n",
    "    xdir, ydir = np.sin(azimuth), np.cos(azimuth)\n",
    "\n",
    "    if resolution is not None:\n",
    "        xdir = xdir * resolution[0]\n",
    "        ydir = ydir * resolution[1]\n",
    "\n",
    "    slope = ydir / xdir\n",
    "    if slope < 1. and slope > -1.:\n",
    "        xdir = 1.\n",
    "        ydir = slope\n",
    "    else:\n",
    "        xdir = 1. / slope\n",
    "        ydir = 1.\n",
    "    return xdir, ydir\n",
    "\n",
    "\n",
    "def _run_shader(sun_zenith, sun_azimuth, elevation_model, resolution_x, resolution_y):\n",
    "\n",
    "    azimuth = np.nanmean(sun_azimuth.astype(np.float32))\n",
    "    zenith = np.nanmean(sun_zenith.astype(np.float32))\n",
    "    resolution = (float(resolution_x), float(resolution_y))\n",
    "    ray_xdir, ray_ydir = rasterize(azimuth, resolution)\n",
    "\n",
    "    # Assume chunking is already done by Dask\n",
    "    ystart = 0\n",
    "    yend = elevation_model.shape[0]\n",
    "\n",
    "    # Make sure inputs have the right data type\n",
    "    zenith = float(zenith)\n",
    "    ray = (float(ray_xdir), float(ray_ydir))\n",
    "    shadow = hillshade(elevation_model.astype(np.float32),\n",
    "                       resolution,\n",
    "                       zenith,\n",
    "                       ray,\n",
    "                       ystart,\n",
    "                       yend)\n",
    "    shadow = shadow.reshape(elevation_model.shape)\n",
    "    shadow[np.isnan(sun_azimuth)] = 255\n",
    "    return shadow\n",
    "\n",
    "\n",
    "def apply_datacube(cube: XarrayDataCube, context: dict) -> XarrayDataCube:\n",
    "    in_xarray = cube.get_array()\n",
    "    sun_zenith = in_xarray.sel({\"bands\": \"sunZenithAngles\"}).values.astype(np.float32)\n",
    "    sun_azimuth = in_xarray.sel({\"bands\": \"sunAzimuthAngles\"}).values.astype(np.float32)\n",
    "    elevation_model = in_xarray.sel({\"bands\": \"DEM\"}).values.astype(np.float32)\n",
    "    res_y = in_xarray.coords[\"y\"][int(len(in_xarray.coords[\"y\"])/2)+1] - in_xarray.coords[\"y\"][int(len(in_xarray.coords[\"y\"])/2)]\n",
    "    res_x = in_xarray.coords[\"x\"][int(len(in_xarray.coords[\"x\"])/2)+1] - in_xarray.coords[\"x\"][int(len(in_xarray.coords[\"x\"])/2)]\n",
    "\n",
    "    shadow = _run_shader(sun_zenith, sun_azimuth, elevation_model, res_x, res_y)\n",
    "    cube.get_array().values[0] = shadow\n",
    "    return cube\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8876989-63b9-4f51-837b-d3b8994f660d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rmgu\\AppData\\Local\\Temp\\ipykernel_36200\\3607831980.py:6: DeprecationWarning: Call to deprecated method `send_job`, use `create_job` instead.\n",
      "  my_job  = hillshaded.send_job(title=\"hillshaded\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00 Job 'j-592ce3aad6df4c4ba512862bd17b6c0f': send 'start'\n",
      "0:00:31 Job 'j-592ce3aad6df4c4ba512862bd17b6c0f': queued (progress N/A)\n",
      "0:00:36 Job 'j-592ce3aad6df4c4ba512862bd17b6c0f': queued (progress N/A)\n",
      "0:00:43 Job 'j-592ce3aad6df4c4ba512862bd17b6c0f': queued (progress N/A)\n",
      "0:00:50 Job 'j-592ce3aad6df4c4ba512862bd17b6c0f': queued (progress N/A)\n",
      "0:01:01 Job 'j-592ce3aad6df4c4ba512862bd17b6c0f': queued (progress N/A)\n",
      "0:01:13 Job 'j-592ce3aad6df4c4ba512862bd17b6c0f': queued (progress N/A)\n",
      "0:01:28 Job 'j-592ce3aad6df4c4ba512862bd17b6c0f': queued (progress N/A)\n",
      "0:01:47 Job 'j-592ce3aad6df4c4ba512862bd17b6c0f': queued (progress N/A)\n",
      "0:02:11 Job 'j-592ce3aad6df4c4ba512862bd17b6c0f': queued (progress N/A)\n",
      "0:02:41 Job 'j-592ce3aad6df4c4ba512862bd17b6c0f': queued (progress N/A)\n",
      "0:03:19 Job 'j-592ce3aad6df4c4ba512862bd17b6c0f': queued (progress N/A)\n",
      "0:04:05 Job 'j-592ce3aad6df4c4ba512862bd17b6c0f': queued (progress N/A)\n",
      "0:05:04 Job 'j-592ce3aad6df4c4ba512862bd17b6c0f': running (progress N/A)\n",
      "0:06:04 Job 'j-592ce3aad6df4c4ba512862bd17b6c0f': running (progress N/A)\n",
      "0:07:04 Job 'j-592ce3aad6df4c4ba512862bd17b6c0f': running (progress N/A)\n",
      "0:08:04 Job 'j-592ce3aad6df4c4ba512862bd17b6c0f': running (progress N/A)\n",
      "0:09:04 Job 'j-592ce3aad6df4c4ba512862bd17b6c0f': running (progress N/A)\n",
      "0:10:04 Job 'j-592ce3aad6df4c4ba512862bd17b6c0f': running (progress N/A)\n",
      "0:11:05 Job 'j-592ce3aad6df4c4ba512862bd17b6c0f': running (progress N/A)\n",
      "0:12:05 Job 'j-592ce3aad6df4c4ba512862bd17b6c0f': running (progress N/A)\n",
      "0:13:05 Job 'j-592ce3aad6df4c4ba512862bd17b6c0f': running (progress N/A)\n"
     ]
    }
   ],
   "source": [
    "process = openeo.UDF(code=udf_code, runtime=\"Python\", data={\"from_parameter\": \"x\"})\n",
    "\n",
    "# Shadow mask replaces the first band (B03) in the merged cube\n",
    "hillshaded = merged_cube.apply(process=process)\n",
    "\n",
    "my_job  = hillshaded.send_job(title=\"hillshaded\")\n",
    "results = my_job.start_and_wait().get_results()\n",
    "results.download_files(\"hilshaded\")\n",
    "\n",
    "hillshaded = hillshaded.rename_labels(\"bands\", [\"hillshade_mask\", \"sunAzimuthAngles\", \"sunZenithAngles\", \"DEM\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b088cea7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <script>\n",
       "    if (!window.customElements || !window.customElements.get('openeo-logs')) {\n",
       "        var el = document.createElement('script');\n",
       "        el.src = \"https://cdn.jsdelivr.net/npm/@openeo/vue-components@2/assets/openeo.min.js\";\n",
       "        document.head.appendChild(el);\n",
       "\n",
       "        var font = document.createElement('font');\n",
       "        font.as = \"font\";\n",
       "        font.type = \"font/woff2\";\n",
       "        font.crossOrigin = true;\n",
       "        font.href = \"https://use.fontawesome.com/releases/v5.13.0/webfonts/fa-solid-900.woff2\"\n",
       "        document.head.appendChild(font);\n",
       "    }\n",
       "    </script>\n",
       "    <openeo-logs>\n",
       "        <script type=\"application/json\">{\"logs\": [{\"id\": \"openeo-yarn-index-1m-2022.10.18-000013/IOY4N4QBMcjMPc-fXQ_t\", \"time\": \"2022-11-02T07:22:30.620Z\", \"level\": \"info\", \"message\": \"Submitting job: ['/opt/venv/lib64/python3.8/site-packages/openeogeotrellis/deploy/submit_batch_job_spark3.sh', 'openEO batch_hillshaded_j-3bf0e39847ee4745803808be3996d20b_user 9b8a05ab83d3da5df69769fd81ba1e5a010e257d100ae51910454badf52bd5d5@egi.eu', '/data/projects/OpenEO/j-3bf0e39847ee4745803808be3996d20b_7n9vhfzr.in', '/data/projects/OpenEO/j-3bf0e39847ee4745803808be3996d20b', 'out', 'log', 'job_metadata.json', 'openeo@VGT.VITO.BE', '/opt/openeo.keytab', 'openeo', '1.1.0', '8G', '2G', '3G', '5', '2', '2G', 'default', 'false', '[]', 'custom_processes.py', '100', '9b8a05ab83d3da5df69769fd81ba1e5a010e257d100ae51910454badf52bd5d5@egi.eu', 'j-3bf0e39847ee4745803808be3996d20b', '0.0', '1', 'default', '/data/projects/OpenEO/j-3bf0e39847ee4745803808be3996d20b_jfwzgqe2.properties']\"}, {\"id\": \"openeo-yarn-index-1m-2022.10.18-000013/HuY4N4QBMcjMPc-fXQ_t\", \"time\": \"2022-11-02T07:22:30.620Z\", \"level\": \"info\", \"message\": \"deemed collection SENTINEL2_L2A_SENTINELHUB AOI (68062397.83501363 m\\u00b2) too small for batch processing (threshold 2500000000 m\\u00b2)\"}, {\"id\": \"openeo-yarn-index-1m-2022.10.18-000013/kfQ4N4QBVWXUH_mWXe_v\", \"time\": \"2022-11-02T07:22:30.620Z\", \"level\": \"info\", \"message\": \"Dry run extracted these source constraints: [(('load_collection', ('SENTINEL2_L2A_SENTINELHUB', ())), {'temporal_extent': ('2022-01-03', '2022-01-08'), 'spatial_extent': {'west': -74.0681076, 'south': 4.68986451, 'east': -73.90597343, 'north': 4.724080996, 'crs': 'epsg:4326'}, 'bands': ['B05', 'sunAzimuthAngles', 'sunZenithAngles']}), (('load_collection', ('COPERNICUS_30', ())), {'temporal_extent': ('2010-01-01', '2030-12-31'), 'spatial_extent': {'west': -74.0681076, 'south': 4.68986451, 'east': -73.90597343, 'north': 4.724080996, 'crs': 'epsg:4326'}, 'process_type': [<ProcessType.GLOBAL_TIME: 4>]}), (('load_collection', ('SENTINEL2_L2A_SENTINELHUB', ())), {'temporal_extent': ('2022-01-03', '2022-01-08'), 'spatial_extent': {'west': -74.0681076, 'south': 4.68986451, 'east': -73.90597343, 'north': 4.724080996, 'crs': 'epsg:4326'}, 'bands': ['B05', 'sunAzimuthAngles', 'sunZenithAngles']})]\"}, {\"id\": \"openeo-yarn-index-1m-2022.10.18-000013/lPQ4N4QBVWXUH_mWXe_v\", \"time\": \"2022-11-02T07:22:30.620Z\", \"level\": \"info\", \"message\": \"deemed collection SENTINEL2_L2A_SENTINELHUB AOI (68062397.83501363 m\\u00b2) too small for batch processing (threshold 2500000000 m\\u00b2)\"}, {\"id\": \"openeo-yarn-index-1m-2022.10.18-000013/kuY4N4QBMcjMPc-fsBsh\", \"time\": \"2022-11-02T07:22:51.622Z\", \"level\": \"info\", \"message\": \"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\\n                                 Dload  Upload   Total   Spent    Left  Speed\\n\\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\\n100  1429  100  1300  100   129  34210   3394 --:--:-- --:--:-- --:--:-- 37605\\n\\n100  1726  100  1597  100   129  16635   1343 --:--:-- --:--:-- --:--:-- 17978\\n{\\\"result\\\": {\\\"count\\\": 3, \\\"truncated\\\": false, \\\"messages\\\": [{\\\"type\\\": \\\"warning\\\", \\\"message\\\": \\\"API Version number was not sent, forward compatibility not guaranteed. Assuming server's API version, 2.231\\\", \\\"code\\\": 13001, \\\"data\\\": {\\\"server_version\\\": \\\"2.231\\\"}, \\\"name\\\": \\\"VersionMissing\\\"}], \\\"result\\\": [{\\\"dn\\\": \\\"uid=openeo,cn=users,cn=accounts,dc=vgt,dc=vito,dc=be\\\", \\\"uid\\\": [\\\"openeo\\\"], \\\"krbcanonicalname\\\": [\\\"openeo@VGT.VITO.BE\\\"], \\\"loginshell\\\": [\\\"/bin/bash\\\"], \\\"uidnumber\\\": [\\\"631703554\\\"], \\\"gidnumber\\\": [\\\"631703554\\\"], \\\"sn\\\": [\\\"TERRA-1378\\\"], \\\"homedirectory\\\": [\\\"/home/openeo\\\"], \\\"mail\\\": [\\\"openeo@vgt.vito.be\\\"], \\\"krbprincipalname\\\": [\\\"openeo@VGT.VITO.BE\\\"], \\\"givenname\\\": [\\\"openeo user\\\"], \\\"nsaccountlock\\\": false}, {\\\"dn\\\": \\\"uid=openeo-dev,cn=users,cn=accounts,dc=vgt,dc=vito,dc=be\\\", \\\"uid\\\": [\\\"openeo-dev\\\"], \\\"krbcanonicalname\\\": [\\\"openeo-dev@VGT.VITO.BE\\\"], \\\"loginshell\\\": [\\\"/bin/bash\\\"], \\\"uidnumber\\\": [\\\"631704057\\\"], \\\"gidnumber\\\": [\\\"631704057\\\"], \\\"sn\\\": [\\\"dev\\\"], \\\"homedirectory\\\": [\\\"/home/openeo-dev\\\"], \\\"mail\\\": [\\\"openeo-dev@vgt.vito.be\\\"], \\\"krbprincipalname\\\": [\\\"openeo-dev@VGT.VITO.BE\\\"], \\\"givenname\\\": [\\\"openeo\\\"], \\\"nsaccountlock\\\": false}, {\\\"dn\\\": \\\"uid=openeo-user1,cn=users,cn=accounts,dc=vgt,dc=vito,dc=be\\\", \\\"uid\\\": [\\\"openeo-user1\\\"], \\\"krbcanonicalname\\\": [\\\"openeo-user1@VGT.VITO.BE\\\"], \\\"loginshell\\\": [\\\"/bin/bash\\\"], \\\"uidnumber\\\": [\\\"631704306\\\"], \\\"gidnumber\\\": [\\\"631704306\\\"], \\\"sn\\\": [\\\"Testuser\\\"], \\\"homedirectory\\\": [\\\"/home/openeo-user1\\\"], \\\"krbprincipalname\\\": [\\\"openeo-user1@VGT.VITO.BE\\\"], \\\"givenname\\\": [\\\"One\\\"], \\\"nsaccountlock\\\": false}], \\\"summary\\\": \\\"3 users matched\\\"}, \\\"version\\\": \\\"4.6.5\\\", \\\"error\\\": null, \\\"id\\\": 0, \\\"principal\\\": \\\"openeo@VGT.VITO.BE\\\"}\\nWARNING: An illegal reflective access operation has occurred\\nWARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\\nWARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\\nWARNING: All illegal access operations will be denied in a future release\\n22/11/02 07:22:32 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm2\\n22/11/02 07:22:33 INFO Client: Requesting a new application from cluster with 139 NodeManagers\\n22/11/02 07:22:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\\n22/11/02 07:22:33 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\\n22/11/02 07:22:33 INFO Configuration: found resource resource-types.xml at file:/etc/hadoop/3.1.4.0-315/0/resource-types.xml\\n22/11/02 07:22:33 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (52224 MB per container)\\n22/11/02 07:22:33 INFO Client: Will allocate AM container, with 10240 MB memory including 2048 MB overhead\\n22/11/02 07:22:33 INFO Client: Setting up container launch context for our AM\\n22/11/02 07:22:33 INFO Client: Setting up the launch environment for our AM container\\n22/11/02 07:22:33 INFO Client: Preparing resources for our AM container\\n22/11/02 07:22:33 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\\n22/11/02 07:22:36 INFO Client: Uploading resource file:/tmp/spark-d5277ae0-9643-4151-aaf6-654d403f8227/__spark_libs__2728227807503784145.zip -> hdfs://hacluster/user/openeo/.sparkStaging/application_1666939039227_12581/__spark_libs__2728227807503784145.zip\\n22/11/02 07:22:40 INFO Client: Uploading resource file:/opt/geotrellis-extensions-static.jar -> hdfs://hacluster/user/openeo/.sparkStaging/application_1666939039227_12581/geotrellis-extensions-static.jar\\n22/11/02 07:22:43 INFO Client: Uploading resource file:/opt/geotrellis-backend-assembly-static.jar -> hdfs://hacluster/user/openeo/.sparkStaging/application_1666939039227_12581/geotrellis-backend-assembly-static.jar\\n22/11/02 07:22:44 INFO Client: Uploading resource file:/opt/layercatalog.json -> hdfs://hacluster/user/openeo/.sparkStaging/application_1666939039227_12581/layercatalog.json\\n22/11/02 07:22:44 INFO Client: Uploading resource file:/data/projects/OpenEO/j-3bf0e39847ee4745803808be3996d20b_7n9vhfzr.in -> hdfs://hacluster/user/openeo/.sparkStaging/application_1666939039227_12581/j-3bf0e39847ee4745803808be3996d20b_7n9vhfzr.in\\n22/11/02 07:22:44 INFO Client: Uploading resource file:/opt/openeo-logging-static.jar -> hdfs://hacluster/user/openeo/.sparkStaging/application_1666939039227_12581/openeo-logging-static.jar\\n22/11/02 07:22:45 INFO Client: Uploading resource file:/opt/client.conf -> hdfs://hacluster/user/openeo/.sparkStaging/application_1666939039227_12581/client.conf\\n22/11/02 07:22:45 INFO Client: Uploading resource file:/opt/http_credentials.json -> hdfs://hacluster/user/openeo/.sparkStaging/application_1666939039227_12581/http_credentials.json\\n22/11/02 07:22:45 INFO Client: Uploading resource file:/opt/venv/lib/python3.8/site-packages/openeogeotrellis/deploy/batch_job.py -> hdfs://hacluster/user/openeo/.sparkStaging/application_1666939039227_12581/batch_job.py\\n22/11/02 07:22:45 INFO Client: Uploading resource file:/usr/local/spark-3.2.0/python/lib/pyspark.zip -> hdfs://hacluster/user/openeo/.sparkStaging/application_1666939039227_12581/pyspark.zip\\n22/11/02 07:22:45 INFO Client: Uploading resource file:/usr/local/spark-3.2.0/python/lib/py4j-0.10.9.2-src.zip -> hdfs://hacluster/user/openeo/.sparkStaging/application_1666939039227_12581/py4j-0.10.9.2-src.zip\\n22/11/02 07:22:45 INFO Client: Uploading resource file:/opt/custom_processes.py -> hdfs://hacluster/user/openeo/.sparkStaging/application_1666939039227_12581/custom_processes.py\\n22/11/02 07:22:45 INFO Client: Uploading resource file:/tmp/spark-d5277ae0-9643-4151-aaf6-654d403f8227/__spark_conf__9071514743495222809.zip -> hdfs://hacluster/user/openeo/.sparkStaging/application_1666939039227_12581/__spark_conf__.zip\\n22/11/02 07:22:46 INFO SecurityManager: Changing view acls to: openeo\\n22/11/02 07:22:46 INFO SecurityManager: Changing modify acls to: openeo\\n22/11/02 07:22:46 INFO SecurityManager: Changing view acls groups to: vito\\n22/11/02 07:22:46 INFO SecurityManager: Changing modify acls groups to: \\n22/11/02 07:22:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(openeo); groups with view permissions: Set(vito); users  with modify permissions: Set(openeo); groups with modify permissions: Set()\\n22/11/02 07:22:46 INFO HiveConf: Found configuration file null\\n22/11/02 07:22:46 INFO HadoopFSDelegationTokenProvider: getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_538235839_1, ugi=openeo (auth:PROXY) via openeo@VGT.VITO.BE (auth:KERBEROS)]] with renewer rm/epod-master1.vgt.vito.be@VGT.VITO.BE\\n22/11/02 07:22:46 INFO DFSClient: Created token for openeo: HDFS_DELEGATION_TOKEN owner=openeo, renewer=yarn, realUser=openeo@VGT.VITO.BE, issueDate=1667373766203, maxDate=1667978566203, sequenceNumber=7127647, masterKeyId=2569 on ha-hdfs:hacluster\\n22/11/02 07:22:46 INFO HadoopFSDelegationTokenProvider: getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_538235839_1, ugi=openeo (auth:PROXY) via openeo@VGT.VITO.BE (auth:KERBEROS)]] with renewer openeo\\n22/11/02 07:22:46 INFO DFSClient: Created token for openeo: HDFS_DELEGATION_TOKEN owner=openeo, renewer=openeo, realUser=openeo@VGT.VITO.BE, issueDate=1667373766222, maxDate=1667978566222, sequenceNumber=7127648, masterKeyId=2569 on ha-hdfs:hacluster\\n22/11/02 07:22:46 INFO HadoopFSDelegationTokenProvider: Renewal interval is 86400043 for token HDFS_DELEGATION_TOKEN\\nObtaining accumulo creds\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:zookeeper.version=3.6.2--803c7f1a12f85978cb049af5e4ef23bd8b688715, built on 09/04/2020 12:44 GMT\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:host.name=epod-openeo-dev.vgt.vito.be\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:java.version=11.0.14\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:java.vendor=Red Hat, Inc.\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-11-openjdk-11.0.14.0.9-2.el8_5.x86_64\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:java.class.path=/usr/local/spark/conf/:/opt/spark3_2_0/jars/jersey-client-2.34.jar:/opt/spark3_2_0/jars/chill-java-0.10.0.jar:/opt/spark3_2_0/jars/xbean-asm9-shaded-4.20.jar:/opt/spark3_2_0/jars/spark-repl_2.12-3.2.0.jar:/opt/spark3_2_0/jars/json4s-scalap_2.12-3.7.0-M11.jar:/opt/spark3_2_0/jars/JTransforms-3.1.jar:/opt/spark3_2_0/jars/hive-common-2.3.9.jar:/opt/spark3_2_0/jars/commons-logging-1.1.3.jar:/opt/spark3_2_0/jars/chill_2.12-0.10.0.jar:/opt/spark3_2_0/jars/spark-graphx_2.12-3.2.0.jar:/opt/spark3_2_0/jars/xz-1.8.jar:/opt/spark3_2_0/jars/stax-api-1.0.1.jar:/opt/spark3_2_0/jars/datanucleus-api-jdo-4.2.4.jar:/opt/spark3_2_0/jars/compress-lzf-1.0.3.jar:/opt/spark3_2_0/jars/commons-crypto-1.1.0.jar:/opt/spark3_2_0/jars/jackson-mapper-asl-1.9.13.jar:/opt/spark3_2_0/jars/spire-util_2.12-0.17.0.jar:/opt/spark3_2_0/jars/janino-3.0.16.jar:/opt/spark3_2_0/jars/spark-launcher_2.12-3.2.0.jar:/opt/spark3_2_0/jars/httpcore-4.4.14.jar:/opt/spark3_2_0/jars/log4j-1.2.17.jar:/opt/spark3_2_0/jars/javolution-5.5.1.jar:/opt/spark3_2_0/jars/orc-shims-1.6.11.jar:/opt/spark3_2_0/jars/jsr305-3.0.0.jar:/opt/spark3_2_0/jars/spark-sketch_2.12-3.2.0.jar:/opt/spark3_2_0/jars/kubernetes-model-metrics-5.4.1.jar:/opt/spark3_2_0/jars/kubernetes-model-events-5.4.1.jar:/opt/spark3_2_0/jars/libfb303-0.9.3.jar:/opt/spark3_2_0/jars/jakarta.validation-api-2.0.2.jar:/opt/spark3_2_0/jars/ST4-4.0.4.jar:/opt/spark3_2_0/jars/hadoop-yarn-server-web-proxy-3.3.1.jar:/opt/spark3_2_0/jars/joda-time-2.10.10.jar:/opt/spark3_2_0/jars/parquet-column-1.12.1.jar:/opt/spark3_2_0/jars/arrow-format-2.0.0.jar:/opt/spark3_2_0/jars/jersey-common-2.34.jar:/opt/spark3_2_0/jars/guava-14.0.1.jar:/opt/spark3_2_0/jars/kubernetes-model-batch-5.4.1.jar:/opt/spark3_2_0/jars/antlr4-runtime-4.8.jar:/opt/spark3_2_0/jars/spire-macros_2.12-0.17.0.jar:/opt/spark3_2_0/jars/jersey-container-servlet-2.34.jar:/opt/spark3_2_0/jars/spark-network-shuffle_2.12-3.2.0.jar:/opt/spark3_2_0/jars/parquet-common-1.12.1.jar:/opt/spark3_2_0/jars/slf4j-log4j12-1.7.30.jar:/opt/spark3_2_0/jars/metrics-graphite-4.2.0.jar:/opt/spark3_2_0/jars/slf4j-api-1.7.30.jar:/opt/spark3_2_0/jars/macro-compat_2.12-1.1.1.jar:/opt/spark3_2_0/jars/commons-compress-1.21.jar:/opt/spark3_2_0/jars/avro-ipc-1.10.2.jar:/opt/spark3_2_0/jars/kubernetes-model-flowcontrol-5.4.1.jar:/opt/spark3_2_0/jars/kubernetes-client-5.4.1.jar:/opt/spark3_2_0/jars/hive-vector-code-gen-2.3.9.jar:/opt/spark3_2_0/jars/spark-hive_2.12-3.2.0.jar:/opt/spark3_2_0/jars/hk2-locator-2.6.1.jar:/opt/spark3_2_0/jars/kubernetes-model-apiextensions-5.4.1.jar:/opt/spark3_2_0/jars/jakarta.ws.rs-api-2.1.6.jar:/opt/spark3_2_0/jars/commons-text-1.6.jar:/opt/spark3_2_0/jars/spark-streaming_2.12-3.2.0.jar:/opt/spark3_2_0/jars/arrow-memory-netty-2.0.0.jar:/opt/spark3_2_0/jars/py4j-0.10.9.2.jar:/opt/spark3_2_0/jars/kubernetes-model-rbac-5.4.1.jar:/opt/spark3_2_0/jars/commons-lang-2.6.jar:/opt/spark3_2_0/jars/scala-library-2.12.15.jar:/opt/spark3_2_0/jars/hive-beeline-2.3.9.jar:/opt/spark3_2_0/jars/activation-1.1.1.jar:/opt/spark3_2_0/jars/parquet-format-structures-1.12.1.jar:/opt/spark3_2_0/jars/snakeyaml-1.27.jar:/opt/spark3_2_0/jars/json-1.8.jar:/opt/spark3_2_0/jars/jersey-hk2-2.34.jar:/opt/spark3_2_0/jars/jakarta.inject-2.6.1.jar:/opt/spark3_2_0/jars/jersey-server-2.34.jar:/opt/spark3_2_0/jars/algebra_2.12-2.0.1.jar:/opt/spark3_2_0/jars/jackson-databind-2.12.3.jar:/opt/spark3_2_0/jars/antlr-runtime-3.5.2.jar:/opt/spark3_2_0/jars/super-csv-2.2.0.jar:/opt/spark3_2_0/jars/spark-tags_2.12-3.2.0-tests.jar:/opt/spark3_2_0/jars/commons-math3-3.4.1.jar:/opt/spark3_2_0/jars/leveldbjni-all-1.8.jar:/opt/spark3_2_0/jars/hive-serde-2.3.9.jar:/opt/spark3_2_0/jars/hive-jdbc-2.3.9.jar:/opt/spark3_2_0/jars/kubernetes-model-apps-5.4.1.jar:/opt/spark3_2_0/jars/kubernetes-model-coordination-5.4.1.jar:/opt/spark3_2_0/jars/protobuf-java-2.5.0.jar:/opt/spark3_2_0/jars/jakarta.xml.bind-api-2.3.2.jar:/opt/spark3_2_0/jars/threeten-extra-1.5.0.jar:/opt/spark3_2_0/jars/spark-mllib-local_2.12-3.2.0.jar:/opt/spark3_2_0/jars/hive-llap-common-2.3.9.jar:/opt/spark3_2_0/jars/metrics-jmx-4.2.0.jar:/opt/spark3_2_0/jars/flatbuffers-java-1.9.0.jar:/opt/spark3_2_0/jars/derby-10.14.2.0.jar:/opt/spark3_2_0/jars/jpam-1.1.jar:/opt/spark3_2_0/jars/audience-annotations-0.5.0.jar:/opt/spark3_2_0/jars/opencsv-2.3.jar:/opt/spark3_2_0/jars/spark-tags_2.12-3.2.0.jar:/opt/spark3_2_0/jars/shapeless_2.12-2.3.3.jar:/opt/spark3_2_0/jars/jackson-annotations-2.12.3.jar:/opt/spark3_2_0/jars/parquet-encoding-1.12.1.jar:/opt/spark3_2_0/jars/jackson-core-2.12.3.jar:/opt/spark3_2_0/jars/jackson-core-asl-1.9.13.jar:/opt/spark3_2_0/jars/jackson-datatype-jsr310-2.11.2.jar:/opt/spark3_2_0/jars/kubernetes-model-common-5.4.1.jar:/opt/spark3_2_0/jars/commons-codec-1.15.jar:/opt/spark3_2_0/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar:/opt/spark3_2_0/jars/hive-shims-scheduler-2.3.9.jar:/opt/spark3_2_0/jars/spark-kubernetes_2.12-3.2.0.jar:/opt/spark3_2_0/jars/hive-shims-0.23-2.3.9.jar:/opt/spark3_2_0/jars/jul-to-slf4j-1.7.30.jar:/opt/spark3_2_0/jars/JLargeArrays-1.5.jar:/opt/spark3_2_0/jars/tink-1.6.0.jar:/opt/spark3_2_0/jars/spark-sql_2.12-3.2.0.jar:/opt/spark3_2_0/jars/transaction-api-1.1.jar:/opt/spark3_2_0/jars/netty-all-4.1.68.Final.jar:/opt/spark3_2_0/jars/jakarta.annotation-api-1.3.5.jar:/opt/spark3_2_0/jars/commons-lang3-3.12.0.jar:/opt/spark3_2_0/jars/hive-cli-2.3.9.jar:/opt/spark3_2_0/jars/zstd-jni-1.5.0-4.jar:/opt/spark3_2_0/jars/scala-collection-compat_2.12-2.1.1.jar:/opt/spark3_2_0/jars/jodd-core-3.5.2.jar:/opt/spark3_2_0/jars/spark-mllib_2.12-3.2.0.jar:/opt/spark3_2_0/jars/commons-net-3.1.jar:/opt/spark3_2_0/jars/univocity-parsers-2.9.1.jar:/opt/spark3_2_0/jars/lapack-2.2.0.jar:/opt/spark3_2_0/jars/snappy-java-1.1.8.4.jar:/opt/spark3_2_0/jars/scala-xml_2.12-1.2.0.jar:/opt/spark3_2_0/jars/zookeeper-3.6.2.jar:/opt/spark3_2_0/jars/arrow-memory-core-2.0.0.jar:/opt/spark3_2_0/jars/okio-1.14.0.jar:/opt/spark3_2_0/jars/rocksdbjni-6.20.3.jar:/opt/spark3_2_0/jars/kubernetes-model-admissionregistration-5.4.1.jar:/opt/spark3_2_0/jars/orc-mapreduce-1.6.11.jar:/opt/spark3_2_0/jars/httpclient-4.5.13.jar:/opt/spark3_2_0/jars/scala-reflect-2.12.15.jar:/opt/spark3_2_0/jars/jta-1.1.jar:/opt/spark3_2_0/jars/json4s-jackson_2.12-3.7.0-M11.jar:/opt/spark3_2_0/jars/orc-core-1.6.11.jar:/opt/spark3_2_0/jars/objenesis-2.6.jar:/opt/spark3_2_0/jars/arpack-2.2.0.jar:/opt/spark3_2_0/jars/kryo-shaded-4.0.2.jar:/opt/spark3_2_0/jars/core-1.1.2.jar:/opt/spark3_2_0/jars/jakarta.servlet-api-4.0.3.jar:/opt/spark3_2_0/jars/jersey-container-servlet-core-2.34.jar:/opt/spark3_2_0/jars/jackson-module-scala_2.12-2.12.3.jar:/opt/spark3_2_0/jars/json4s-ast_2.12-3.7.0-M11.jar:/opt/spark3_2_0/jars/RoaringBitmap-0.9.0.jar:/opt/spark3_2_0/jars/kubernetes-model-node-5.4.1.jar:/opt/spark3_2_0/jars/pyrolite-4.30.jar:/opt/spark3_2_0/jars/metrics-json-4.2.0.jar:/opt/spark3_2_0/jars/jline-2.14.6.jar:/opt/spark3_2_0/jars/hk2-utils-2.6.1.jar:/opt/spark3_2_0/jars/hadoop-shaded-guava-1.1.1.jar:/opt/spark3_2_0/jars/breeze-macros_2.12-1.2.jar:/opt/spark3_2_0/jars/spark-mesos_2.12-3.2.0.jar:/opt/spark3_2_0/jars/curator-framework-2.13.0.jar:/opt/spark3_2_0/jars/bonecp-0.8.0.RELEASE.jar:/opt/spark3_2_0/jars/hive-storage-api-2.7.2.jar:/opt/spark3_2_0/jars/HikariCP-2.5.1.jar:/opt/spark3_2_0/jars/curator-recipes-2.13.0.jar:/opt/spark3_2_0/jars/datanucleus-rdbms-4.1.19.jar:/opt/spark3_2_0/jars/libthrift-0.12.0.jar:/opt/spark3_2_0/jars/zjsonpatch-0.3.0.jar:/opt/spark3_2_0/jars/kubernetes-model-discovery-5.4.1.jar:/opt/spark3_2_0/jars/velocity-1.5.jar:/opt/spark3_2_0/jars/kubernetes-model-policy-5.4.1.jar:/opt/spark3_2_0/jars/kubernetes-model-extensions-5.4.1.jar:/opt/spark3_2_0/jars/hive-shims-common-2.3.9.jar:/opt/spark3_2_0/jars/jaxb-api-2.2.11.jar:/opt/spark3_2_0/jars/jackson-dataformat-yaml-2.12.3.jar:/opt/spark3_2_0/jars/curator-client-2.13.0.jar:/opt/spark3_2_0/jars/spark-kvstore_2.12-3.2.0.jar:/opt/spark3_2_0/jars/json4s-core_2.12-3.7.0-M11.jar:/opt/spark3_2_0/jars/spark-network-common_2.12-3.2.0.jar:/opt/spark3_2_0/jars/kubernetes-model-networking-5.4.1.jar:/opt/spark3_2_0/jars/spark-unsafe_2.12-3.2.0.jar:/opt/spark3_2_0/jars/hive-metastore-2.3.9.jar:/opt/spark3_2_0/jars/javassist-3.25.0-GA.jar:/opt/spark3_2_0/jars/commons-io-2.8.0.jar:/opt/spark3_2_0/jars/parquet-hadoop-1.12.1.jar:/opt/spark3_2_0/jars/arpack_combined_all-0.1.jar:/opt/spark3_2_0/jars/blas-2.2.0.jar:/opt/spark3_2_0/jars/spire_2.12-0.17.0.jar:/opt/spark3_2_0/jars/stream-2.9.6.jar:/opt/spark3_2_0/jars/gson-2.2.4.jar:/opt/spark3_2_0/jars/cats-kernel_2.12-2.1.1.jar:/opt/spark3_2_0/jars/htrace-core4-4.1.0-incubating.jar:/opt/spark3_2_0/jars/arrow-vector-2.0.0.jar:/opt/spark3_2_0/jars/avro-1.10.2.jar:/opt/spark3_2_0/jars/datanucleus-core-4.1.17.jar:/opt/spark3_2_0/jars/commons-cli-1.2.jar:/opt/spark3_2_0/jars/hive-exec-2.3.9-core.jar:/opt/spark3_2_0/jars/breeze_2.12-1.2.jar:/opt/spark3_2_0/jars/avro-mapred-1.10.2.jar:/opt/spark3_2_0/jars/spark-hive-thriftserver_2.12-3.2.0.jar:/opt/spark3_2_0/jars/aircompressor-0.21.jar:/opt/spark3_2_0/jars/metrics-jvm-4.2.0.jar:/opt/spark3_2_0/jars/mesos-1.4.0-shaded-protobuf.jar:/opt/spark3_2_0/jars/automaton-1.11-8.jar:/opt/spark3_2_0/jars/zookeeper-jute-3.6.2.jar:/opt/spark3_2_0/jars/lz4-java-1.7.1.jar:/opt/spark3_2_0/jars/istack-commons-runtime-3.0.8.jar:/opt/spark3_2_0/jars/scala-compiler-2.12.15.jar:/opt/spark3_2_0/jars/paranamer-2.8.jar:/opt/spark3_2_0/jars/kubernetes-model-certificates-5.4.1.jar:/opt/spark3_2_0/jars/ivy-2.5.0.jar:/opt/spark3_2_0/jars/logging-interceptor-3.12.12.jar:/opt/spark3_2_0/jars/commons-dbcp-1.4.jar:/opt/spark3_2_0/jars/kubernetes-model-core-5.4.1.jar:/opt/spark3_2_0/jars/okhttp-3.12.12.jar:/opt/spark3_2_0/jars/shims-0.9.0.jar:/opt/spark3_2_0/jars/oro-2.0.8.jar:/opt/spark3_2_0/jars/parquet-jackson-1.12.1.jar:/opt/spark3_2_0/jars/annotations-17.0.0.jar:/opt/spark3_2_0/jars/hive-shims-2.3.9.jar:/opt/spark3_2_0/jars/jcl-over-slf4j-1.7.30.jar:/opt/spark3_2_0/jars/kubernetes-model-storageclass-5.4.1.jar:/opt/spark3_2_0/jars/commons-collections-3.2.2.jar:/opt/spark3_2_0/jars/commons-compiler-3.0.16.jar:/opt/spark3_2_0/jars/hadoop-client-runtime-3.3.1.jar:/opt/spark3_2_0/jars/hk2-api-2.6.1.jar:/opt/spark3_2_0/jars/aopalliance-repackaged-2.6.1.jar:/opt/spark3_2_0/jars/minlog-1.3.0.jar:/opt/spark3_2_0/jars/osgi-resource-locator-1.0.3.jar:/opt/spark3_2_0/jars/jdo-api-3.0.1.jar:/opt/spark3_2_0/jars/hive-service-rpc-3.1.2.jar:/opt/spark3_2_0/jars/scala-parser-combinators_2.12-1.1.2.jar:/opt/spark3_2_0/jars/commons-pool-1.5.4.jar:/opt/spark3_2_0/jars/kubernetes-model-scheduling-5.4.1.jar:/opt/spark3_2_0/jars/spark-core_2.12-3.2.0.jar:/opt/spark3_2_0/jars/generex-1.0.2.jar:/opt/spark3_2_0/jars/javax.jdo-3.2.0-m3.jar:/opt/spark3_2_0/jars/metrics-core-4.2.0.jar:/opt/spark3_2_0/jars/spark-catalyst_2.12-3.2.0.jar:/opt/spark3_2_0/jars/spark-yarn_2.12-3.2.0.jar:/opt/spark3_2_0/jars/jaxb-runtime-2.3.2.jar:/opt/spark3_2_0/jars/kubernetes-model-autoscaling-5.4.1.jar:/opt/spark3_2_0/jars/spire-platform_2.12-0.17.0.jar:/opt/spark3_2_0/jars/hadoop-client-api-3.3.1.jar:/etc/hadoop/conf/\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:java.library.path=/opt/venv/lib64:/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:java.io.tmpdir=/tmp\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:java.compiler=<NA>\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:os.name=Linux\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:os.arch=amd64\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:os.version=3.10.0-1062.12.1.el7.x86_64\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:user.name=openeo\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:user.home=/home/openeo\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:user.dir=/opt\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:os.memory.free=1131MB\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:os.memory.max=24028MB\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:os.memory.total=1512MB\\n22/11/02 07:22:47 INFO ZooKeeper: Initiating client connection, connectString=epod-master1.vgt.vito.be:2181,epod-master2.vgt.vito.be:2181,epod-master3.vgt.vito.be:2181 sessionTimeout=50000 watcher=org.apache.accumulo.fate.zookeeper.ZooSession$ZooWatcher@615e83ac\\n22/11/02 07:22:47 INFO X509Util: Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation\\n22/11/02 07:22:47 INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\\n22/11/02 07:22:47 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\\n22/11/02 07:22:47 INFO ClientCnxn: Opening socket connection to server epod-master2.vgt.vito.be/192.168.207.57:2181.\\n22/11/02 07:22:47 INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\\n22/11/02 07:22:47 INFO ClientCnxn: Socket connection established, initiating session, client: /192.168.207.55:46202, server: epod-master2.vgt.vito.be/192.168.207.57:2181\\n22/11/02 07:22:47 INFO ClientCnxn: Session establishment complete on server epod-master2.vgt.vito.be/192.168.207.57:2181, session id = 0x2841d4f04f7c698, negotiated timeout = 40000\\nCreated token for: AccumuloDelegationToken-952b4740-0738-46d1-a031-11a0c00a801d , expires: 1667395367772\\n22/11/02 07:22:47 INFO Client: Submitting application application_1666939039227_12581 to ResourceManager\\n22/11/02 07:22:48 INFO YarnClientImpl: Submitted application application_1666939039227_12581\\n22/11/02 07:22:48 INFO Client: Application report for application_1666939039227_12581 (state: ACCEPTED)\\n22/11/02 07:22:48 INFO Client: \\n\\t client token: N/A\\n\\t diagnostics: [Wed Nov 02 08:22:47 +0100 2022] Application is Activated, waiting for resources to be assigned for AM.  Last Node which was processed for the application : epod018.vgt.vito.be:45454 ( Partition : [], Total resource : <memory:118784, vCores:28>, Available resource : <memory:4608, vCores:4> ). Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:17182720, vCores:4800> ; Queue's Absolute capacity = 23.0 % ; Queue's Absolute used capacity = 18.270834 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:3952025, vCores:1104> ; Queue's used capacity (absolute resource) = <memory:1661952, vCores:877> ; Queue's max capacity (absolute resource) = <memory:17182720, vCores:4800> ; \\n\\t ApplicationMaster host: N/A\\n\\t ApplicationMaster RPC port: -1\\n\\t queue: default\\n\\t start time: 1667373767810\\n\\t final status: UNDEFINED\\n\\t tracking URL: https://epod-master2.vgt.vito.be:8090/proxy/application_1666939039227_12581/\\n\\t user: openeo\\n22/11/02 07:22:48 INFO ShutdownHookManager: Shutdown hook called\\n22/11/02 07:22:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-d7ebb0ef-d05d-4160-beb6-9c0f3196a6ea\\n22/11/02 07:22:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-d5277ae0-9643-4151-aaf6-654d403f8227\\n\"}, {\"id\": \"openeo-yarn-index-1m-2022.10.18-000013/k-Y4N4QBMcjMPc-fsBsh\", \"time\": \"2022-11-02T07:22:51.622Z\", \"level\": \"info\", \"message\": \"mapped job_id j-3bf0e39847ee4745803808be3996d20b to application ID application_1666939039227_12581\"}, {\"id\": \"openeo-yarn-index-1m-2022.10.18-000013/lvY9N4QBVWXUH_mWjBF5\", \"time\": \"2022-11-02T07:28:08.968Z\", \"level\": \"info\", \"message\": \"changed job j-3bf0e39847ee4745803808be3996d20b status from queued to error\"}, {\"id\": \"openeo-yarn-index-1m-2022.10.18-000013/l_Y9N4QBVWXUH_mWjBF5\", \"time\": \"2022-11-02T07:28:08.984Z\", \"level\": \"warning\", \"message\": \"Could not derive result metadata from /data/projects/OpenEO/j-3bf0e39847ee4745803808be3996d20b/job_metadata.json\"}, {\"id\": \"openeo-yarn-index-1m-2022.10.18-000013/mPY9N4QBVWXUH_mWjBF5\", \"time\": \"2022-11-02T07:28:09.092Z\", \"level\": \"info\", \"message\": \"marked j-3bf0e39847ee4745803808be3996d20b as done\"}, {\"id\": \"error\", \"level\": \"error\", \"message\": \"Traceback (most recent call last):\\n  File \\\"/opt/venv/lib64/python3.8/site-packages/openeogeotrellis/backend.py\\\", line 1913, in get_log_entries\\n    with (self.get_job_output_dir(job_id) / \\\"log\\\").open('r') as f:\\n  File \\\"/usr/lib64/python3.8/pathlib.py\\\", line 1221, in open\\n    return io.open(self, mode, buffering, encoding, errors, newline,\\n  File \\\"/usr/lib64/python3.8/pathlib.py\\\", line 1077, in _opener\\n    return self._accessor.open(self, flags, mode)\\nFileNotFoundError: [Errno 2] No such file or directory: '/data/projects/OpenEO/j-3bf0e39847ee4745803808be3996d20b/log'\\n\"}]}</script>\n",
       "    </openeo-logs>\n",
       "    "
      ],
      "text/plain": [
       "[{'id': 'openeo-yarn-index-1m-2022.10.18-000013/IOY4N4QBMcjMPc-fXQ_t',\n",
       "  'time': '2022-11-02T07:22:30.620Z',\n",
       "  'level': 'info',\n",
       "  'message': \"Submitting job: ['/opt/venv/lib64/python3.8/site-packages/openeogeotrellis/deploy/submit_batch_job_spark3.sh', 'openEO batch_hillshaded_j-3bf0e39847ee4745803808be3996d20b_user 9b8a05ab83d3da5df69769fd81ba1e5a010e257d100ae51910454badf52bd5d5@egi.eu', '/data/projects/OpenEO/j-3bf0e39847ee4745803808be3996d20b_7n9vhfzr.in', '/data/projects/OpenEO/j-3bf0e39847ee4745803808be3996d20b', 'out', 'log', 'job_metadata.json', 'openeo@VGT.VITO.BE', '/opt/openeo.keytab', 'openeo', '1.1.0', '8G', '2G', '3G', '5', '2', '2G', 'default', 'false', '[]', 'custom_processes.py', '100', '9b8a05ab83d3da5df69769fd81ba1e5a010e257d100ae51910454badf52bd5d5@egi.eu', 'j-3bf0e39847ee4745803808be3996d20b', '0.0', '1', 'default', '/data/projects/OpenEO/j-3bf0e39847ee4745803808be3996d20b_jfwzgqe2.properties']\"},\n",
       " {'id': 'openeo-yarn-index-1m-2022.10.18-000013/HuY4N4QBMcjMPc-fXQ_t',\n",
       "  'time': '2022-11-02T07:22:30.620Z',\n",
       "  'level': 'info',\n",
       "  'message': 'deemed collection SENTINEL2_L2A_SENTINELHUB AOI (68062397.83501363 m²) too small for batch processing (threshold 2500000000 m²)'},\n",
       " {'id': 'openeo-yarn-index-1m-2022.10.18-000013/kfQ4N4QBVWXUH_mWXe_v',\n",
       "  'time': '2022-11-02T07:22:30.620Z',\n",
       "  'level': 'info',\n",
       "  'message': \"Dry run extracted these source constraints: [(('load_collection', ('SENTINEL2_L2A_SENTINELHUB', ())), {'temporal_extent': ('2022-01-03', '2022-01-08'), 'spatial_extent': {'west': -74.0681076, 'south': 4.68986451, 'east': -73.90597343, 'north': 4.724080996, 'crs': 'epsg:4326'}, 'bands': ['B05', 'sunAzimuthAngles', 'sunZenithAngles']}), (('load_collection', ('COPERNICUS_30', ())), {'temporal_extent': ('2010-01-01', '2030-12-31'), 'spatial_extent': {'west': -74.0681076, 'south': 4.68986451, 'east': -73.90597343, 'north': 4.724080996, 'crs': 'epsg:4326'}, 'process_type': [<ProcessType.GLOBAL_TIME: 4>]}), (('load_collection', ('SENTINEL2_L2A_SENTINELHUB', ())), {'temporal_extent': ('2022-01-03', '2022-01-08'), 'spatial_extent': {'west': -74.0681076, 'south': 4.68986451, 'east': -73.90597343, 'north': 4.724080996, 'crs': 'epsg:4326'}, 'bands': ['B05', 'sunAzimuthAngles', 'sunZenithAngles']})]\"},\n",
       " {'id': 'openeo-yarn-index-1m-2022.10.18-000013/lPQ4N4QBVWXUH_mWXe_v',\n",
       "  'time': '2022-11-02T07:22:30.620Z',\n",
       "  'level': 'info',\n",
       "  'message': 'deemed collection SENTINEL2_L2A_SENTINELHUB AOI (68062397.83501363 m²) too small for batch processing (threshold 2500000000 m²)'},\n",
       " {'id': 'openeo-yarn-index-1m-2022.10.18-000013/kuY4N4QBMcjMPc-fsBsh',\n",
       "  'time': '2022-11-02T07:22:51.622Z',\n",
       "  'level': 'info',\n",
       "  'message': '  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\\n                                 Dload  Upload   Total   Spent    Left  Speed\\n\\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\\n100  1429  100  1300  100   129  34210   3394 --:--:-- --:--:-- --:--:-- 37605\\n\\n100  1726  100  1597  100   129  16635   1343 --:--:-- --:--:-- --:--:-- 17978\\n{\"result\": {\"count\": 3, \"truncated\": false, \"messages\": [{\"type\": \"warning\", \"message\": \"API Version number was not sent, forward compatibility not guaranteed. Assuming server\\'s API version, 2.231\", \"code\": 13001, \"data\": {\"server_version\": \"2.231\"}, \"name\": \"VersionMissing\"}], \"result\": [{\"dn\": \"uid=openeo,cn=users,cn=accounts,dc=vgt,dc=vito,dc=be\", \"uid\": [\"openeo\"], \"krbcanonicalname\": [\"openeo@VGT.VITO.BE\"], \"loginshell\": [\"/bin/bash\"], \"uidnumber\": [\"631703554\"], \"gidnumber\": [\"631703554\"], \"sn\": [\"TERRA-1378\"], \"homedirectory\": [\"/home/openeo\"], \"mail\": [\"openeo@vgt.vito.be\"], \"krbprincipalname\": [\"openeo@VGT.VITO.BE\"], \"givenname\": [\"openeo user\"], \"nsaccountlock\": false}, {\"dn\": \"uid=openeo-dev,cn=users,cn=accounts,dc=vgt,dc=vito,dc=be\", \"uid\": [\"openeo-dev\"], \"krbcanonicalname\": [\"openeo-dev@VGT.VITO.BE\"], \"loginshell\": [\"/bin/bash\"], \"uidnumber\": [\"631704057\"], \"gidnumber\": [\"631704057\"], \"sn\": [\"dev\"], \"homedirectory\": [\"/home/openeo-dev\"], \"mail\": [\"openeo-dev@vgt.vito.be\"], \"krbprincipalname\": [\"openeo-dev@VGT.VITO.BE\"], \"givenname\": [\"openeo\"], \"nsaccountlock\": false}, {\"dn\": \"uid=openeo-user1,cn=users,cn=accounts,dc=vgt,dc=vito,dc=be\", \"uid\": [\"openeo-user1\"], \"krbcanonicalname\": [\"openeo-user1@VGT.VITO.BE\"], \"loginshell\": [\"/bin/bash\"], \"uidnumber\": [\"631704306\"], \"gidnumber\": [\"631704306\"], \"sn\": [\"Testuser\"], \"homedirectory\": [\"/home/openeo-user1\"], \"krbprincipalname\": [\"openeo-user1@VGT.VITO.BE\"], \"givenname\": [\"One\"], \"nsaccountlock\": false}], \"summary\": \"3 users matched\"}, \"version\": \"4.6.5\", \"error\": null, \"id\": 0, \"principal\": \"openeo@VGT.VITO.BE\"}\\nWARNING: An illegal reflective access operation has occurred\\nWARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.2.0/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\\nWARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\\nWARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\\nWARNING: All illegal access operations will be denied in a future release\\n22/11/02 07:22:32 INFO ConfiguredRMFailoverProxyProvider: Failing over to rm2\\n22/11/02 07:22:33 INFO Client: Requesting a new application from cluster with 139 NodeManagers\\n22/11/02 07:22:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\\n22/11/02 07:22:33 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\\n22/11/02 07:22:33 INFO Configuration: found resource resource-types.xml at file:/etc/hadoop/3.1.4.0-315/0/resource-types.xml\\n22/11/02 07:22:33 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (52224 MB per container)\\n22/11/02 07:22:33 INFO Client: Will allocate AM container, with 10240 MB memory including 2048 MB overhead\\n22/11/02 07:22:33 INFO Client: Setting up container launch context for our AM\\n22/11/02 07:22:33 INFO Client: Setting up the launch environment for our AM container\\n22/11/02 07:22:33 INFO Client: Preparing resources for our AM container\\n22/11/02 07:22:33 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\\n22/11/02 07:22:36 INFO Client: Uploading resource file:/tmp/spark-d5277ae0-9643-4151-aaf6-654d403f8227/__spark_libs__2728227807503784145.zip -> hdfs://hacluster/user/openeo/.sparkStaging/application_1666939039227_12581/__spark_libs__2728227807503784145.zip\\n22/11/02 07:22:40 INFO Client: Uploading resource file:/opt/geotrellis-extensions-static.jar -> hdfs://hacluster/user/openeo/.sparkStaging/application_1666939039227_12581/geotrellis-extensions-static.jar\\n22/11/02 07:22:43 INFO Client: Uploading resource file:/opt/geotrellis-backend-assembly-static.jar -> hdfs://hacluster/user/openeo/.sparkStaging/application_1666939039227_12581/geotrellis-backend-assembly-static.jar\\n22/11/02 07:22:44 INFO Client: Uploading resource file:/opt/layercatalog.json -> hdfs://hacluster/user/openeo/.sparkStaging/application_1666939039227_12581/layercatalog.json\\n22/11/02 07:22:44 INFO Client: Uploading resource file:/data/projects/OpenEO/j-3bf0e39847ee4745803808be3996d20b_7n9vhfzr.in -> hdfs://hacluster/user/openeo/.sparkStaging/application_1666939039227_12581/j-3bf0e39847ee4745803808be3996d20b_7n9vhfzr.in\\n22/11/02 07:22:44 INFO Client: Uploading resource file:/opt/openeo-logging-static.jar -> hdfs://hacluster/user/openeo/.sparkStaging/application_1666939039227_12581/openeo-logging-static.jar\\n22/11/02 07:22:45 INFO Client: Uploading resource file:/opt/client.conf -> hdfs://hacluster/user/openeo/.sparkStaging/application_1666939039227_12581/client.conf\\n22/11/02 07:22:45 INFO Client: Uploading resource file:/opt/http_credentials.json -> hdfs://hacluster/user/openeo/.sparkStaging/application_1666939039227_12581/http_credentials.json\\n22/11/02 07:22:45 INFO Client: Uploading resource file:/opt/venv/lib/python3.8/site-packages/openeogeotrellis/deploy/batch_job.py -> hdfs://hacluster/user/openeo/.sparkStaging/application_1666939039227_12581/batch_job.py\\n22/11/02 07:22:45 INFO Client: Uploading resource file:/usr/local/spark-3.2.0/python/lib/pyspark.zip -> hdfs://hacluster/user/openeo/.sparkStaging/application_1666939039227_12581/pyspark.zip\\n22/11/02 07:22:45 INFO Client: Uploading resource file:/usr/local/spark-3.2.0/python/lib/py4j-0.10.9.2-src.zip -> hdfs://hacluster/user/openeo/.sparkStaging/application_1666939039227_12581/py4j-0.10.9.2-src.zip\\n22/11/02 07:22:45 INFO Client: Uploading resource file:/opt/custom_processes.py -> hdfs://hacluster/user/openeo/.sparkStaging/application_1666939039227_12581/custom_processes.py\\n22/11/02 07:22:45 INFO Client: Uploading resource file:/tmp/spark-d5277ae0-9643-4151-aaf6-654d403f8227/__spark_conf__9071514743495222809.zip -> hdfs://hacluster/user/openeo/.sparkStaging/application_1666939039227_12581/__spark_conf__.zip\\n22/11/02 07:22:46 INFO SecurityManager: Changing view acls to: openeo\\n22/11/02 07:22:46 INFO SecurityManager: Changing modify acls to: openeo\\n22/11/02 07:22:46 INFO SecurityManager: Changing view acls groups to: vito\\n22/11/02 07:22:46 INFO SecurityManager: Changing modify acls groups to: \\n22/11/02 07:22:46 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(openeo); groups with view permissions: Set(vito); users  with modify permissions: Set(openeo); groups with modify permissions: Set()\\n22/11/02 07:22:46 INFO HiveConf: Found configuration file null\\n22/11/02 07:22:46 INFO HadoopFSDelegationTokenProvider: getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_538235839_1, ugi=openeo (auth:PROXY) via openeo@VGT.VITO.BE (auth:KERBEROS)]] with renewer rm/epod-master1.vgt.vito.be@VGT.VITO.BE\\n22/11/02 07:22:46 INFO DFSClient: Created token for openeo: HDFS_DELEGATION_TOKEN owner=openeo, renewer=yarn, realUser=openeo@VGT.VITO.BE, issueDate=1667373766203, maxDate=1667978566203, sequenceNumber=7127647, masterKeyId=2569 on ha-hdfs:hacluster\\n22/11/02 07:22:46 INFO HadoopFSDelegationTokenProvider: getting token for: DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_538235839_1, ugi=openeo (auth:PROXY) via openeo@VGT.VITO.BE (auth:KERBEROS)]] with renewer openeo\\n22/11/02 07:22:46 INFO DFSClient: Created token for openeo: HDFS_DELEGATION_TOKEN owner=openeo, renewer=openeo, realUser=openeo@VGT.VITO.BE, issueDate=1667373766222, maxDate=1667978566222, sequenceNumber=7127648, masterKeyId=2569 on ha-hdfs:hacluster\\n22/11/02 07:22:46 INFO HadoopFSDelegationTokenProvider: Renewal interval is 86400043 for token HDFS_DELEGATION_TOKEN\\nObtaining accumulo creds\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:zookeeper.version=3.6.2--803c7f1a12f85978cb049af5e4ef23bd8b688715, built on 09/04/2020 12:44 GMT\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:host.name=epod-openeo-dev.vgt.vito.be\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:java.version=11.0.14\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:java.vendor=Red Hat, Inc.\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:java.home=/usr/lib/jvm/java-11-openjdk-11.0.14.0.9-2.el8_5.x86_64\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:java.class.path=/usr/local/spark/conf/:/opt/spark3_2_0/jars/jersey-client-2.34.jar:/opt/spark3_2_0/jars/chill-java-0.10.0.jar:/opt/spark3_2_0/jars/xbean-asm9-shaded-4.20.jar:/opt/spark3_2_0/jars/spark-repl_2.12-3.2.0.jar:/opt/spark3_2_0/jars/json4s-scalap_2.12-3.7.0-M11.jar:/opt/spark3_2_0/jars/JTransforms-3.1.jar:/opt/spark3_2_0/jars/hive-common-2.3.9.jar:/opt/spark3_2_0/jars/commons-logging-1.1.3.jar:/opt/spark3_2_0/jars/chill_2.12-0.10.0.jar:/opt/spark3_2_0/jars/spark-graphx_2.12-3.2.0.jar:/opt/spark3_2_0/jars/xz-1.8.jar:/opt/spark3_2_0/jars/stax-api-1.0.1.jar:/opt/spark3_2_0/jars/datanucleus-api-jdo-4.2.4.jar:/opt/spark3_2_0/jars/compress-lzf-1.0.3.jar:/opt/spark3_2_0/jars/commons-crypto-1.1.0.jar:/opt/spark3_2_0/jars/jackson-mapper-asl-1.9.13.jar:/opt/spark3_2_0/jars/spire-util_2.12-0.17.0.jar:/opt/spark3_2_0/jars/janino-3.0.16.jar:/opt/spark3_2_0/jars/spark-launcher_2.12-3.2.0.jar:/opt/spark3_2_0/jars/httpcore-4.4.14.jar:/opt/spark3_2_0/jars/log4j-1.2.17.jar:/opt/spark3_2_0/jars/javolution-5.5.1.jar:/opt/spark3_2_0/jars/orc-shims-1.6.11.jar:/opt/spark3_2_0/jars/jsr305-3.0.0.jar:/opt/spark3_2_0/jars/spark-sketch_2.12-3.2.0.jar:/opt/spark3_2_0/jars/kubernetes-model-metrics-5.4.1.jar:/opt/spark3_2_0/jars/kubernetes-model-events-5.4.1.jar:/opt/spark3_2_0/jars/libfb303-0.9.3.jar:/opt/spark3_2_0/jars/jakarta.validation-api-2.0.2.jar:/opt/spark3_2_0/jars/ST4-4.0.4.jar:/opt/spark3_2_0/jars/hadoop-yarn-server-web-proxy-3.3.1.jar:/opt/spark3_2_0/jars/joda-time-2.10.10.jar:/opt/spark3_2_0/jars/parquet-column-1.12.1.jar:/opt/spark3_2_0/jars/arrow-format-2.0.0.jar:/opt/spark3_2_0/jars/jersey-common-2.34.jar:/opt/spark3_2_0/jars/guava-14.0.1.jar:/opt/spark3_2_0/jars/kubernetes-model-batch-5.4.1.jar:/opt/spark3_2_0/jars/antlr4-runtime-4.8.jar:/opt/spark3_2_0/jars/spire-macros_2.12-0.17.0.jar:/opt/spark3_2_0/jars/jersey-container-servlet-2.34.jar:/opt/spark3_2_0/jars/spark-network-shuffle_2.12-3.2.0.jar:/opt/spark3_2_0/jars/parquet-common-1.12.1.jar:/opt/spark3_2_0/jars/slf4j-log4j12-1.7.30.jar:/opt/spark3_2_0/jars/metrics-graphite-4.2.0.jar:/opt/spark3_2_0/jars/slf4j-api-1.7.30.jar:/opt/spark3_2_0/jars/macro-compat_2.12-1.1.1.jar:/opt/spark3_2_0/jars/commons-compress-1.21.jar:/opt/spark3_2_0/jars/avro-ipc-1.10.2.jar:/opt/spark3_2_0/jars/kubernetes-model-flowcontrol-5.4.1.jar:/opt/spark3_2_0/jars/kubernetes-client-5.4.1.jar:/opt/spark3_2_0/jars/hive-vector-code-gen-2.3.9.jar:/opt/spark3_2_0/jars/spark-hive_2.12-3.2.0.jar:/opt/spark3_2_0/jars/hk2-locator-2.6.1.jar:/opt/spark3_2_0/jars/kubernetes-model-apiextensions-5.4.1.jar:/opt/spark3_2_0/jars/jakarta.ws.rs-api-2.1.6.jar:/opt/spark3_2_0/jars/commons-text-1.6.jar:/opt/spark3_2_0/jars/spark-streaming_2.12-3.2.0.jar:/opt/spark3_2_0/jars/arrow-memory-netty-2.0.0.jar:/opt/spark3_2_0/jars/py4j-0.10.9.2.jar:/opt/spark3_2_0/jars/kubernetes-model-rbac-5.4.1.jar:/opt/spark3_2_0/jars/commons-lang-2.6.jar:/opt/spark3_2_0/jars/scala-library-2.12.15.jar:/opt/spark3_2_0/jars/hive-beeline-2.3.9.jar:/opt/spark3_2_0/jars/activation-1.1.1.jar:/opt/spark3_2_0/jars/parquet-format-structures-1.12.1.jar:/opt/spark3_2_0/jars/snakeyaml-1.27.jar:/opt/spark3_2_0/jars/json-1.8.jar:/opt/spark3_2_0/jars/jersey-hk2-2.34.jar:/opt/spark3_2_0/jars/jakarta.inject-2.6.1.jar:/opt/spark3_2_0/jars/jersey-server-2.34.jar:/opt/spark3_2_0/jars/algebra_2.12-2.0.1.jar:/opt/spark3_2_0/jars/jackson-databind-2.12.3.jar:/opt/spark3_2_0/jars/antlr-runtime-3.5.2.jar:/opt/spark3_2_0/jars/super-csv-2.2.0.jar:/opt/spark3_2_0/jars/spark-tags_2.12-3.2.0-tests.jar:/opt/spark3_2_0/jars/commons-math3-3.4.1.jar:/opt/spark3_2_0/jars/leveldbjni-all-1.8.jar:/opt/spark3_2_0/jars/hive-serde-2.3.9.jar:/opt/spark3_2_0/jars/hive-jdbc-2.3.9.jar:/opt/spark3_2_0/jars/kubernetes-model-apps-5.4.1.jar:/opt/spark3_2_0/jars/kubernetes-model-coordination-5.4.1.jar:/opt/spark3_2_0/jars/protobuf-java-2.5.0.jar:/opt/spark3_2_0/jars/jakarta.xml.bind-api-2.3.2.jar:/opt/spark3_2_0/jars/threeten-extra-1.5.0.jar:/opt/spark3_2_0/jars/spark-mllib-local_2.12-3.2.0.jar:/opt/spark3_2_0/jars/hive-llap-common-2.3.9.jar:/opt/spark3_2_0/jars/metrics-jmx-4.2.0.jar:/opt/spark3_2_0/jars/flatbuffers-java-1.9.0.jar:/opt/spark3_2_0/jars/derby-10.14.2.0.jar:/opt/spark3_2_0/jars/jpam-1.1.jar:/opt/spark3_2_0/jars/audience-annotations-0.5.0.jar:/opt/spark3_2_0/jars/opencsv-2.3.jar:/opt/spark3_2_0/jars/spark-tags_2.12-3.2.0.jar:/opt/spark3_2_0/jars/shapeless_2.12-2.3.3.jar:/opt/spark3_2_0/jars/jackson-annotations-2.12.3.jar:/opt/spark3_2_0/jars/parquet-encoding-1.12.1.jar:/opt/spark3_2_0/jars/jackson-core-2.12.3.jar:/opt/spark3_2_0/jars/jackson-core-asl-1.9.13.jar:/opt/spark3_2_0/jars/jackson-datatype-jsr310-2.11.2.jar:/opt/spark3_2_0/jars/kubernetes-model-common-5.4.1.jar:/opt/spark3_2_0/jars/commons-codec-1.15.jar:/opt/spark3_2_0/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar:/opt/spark3_2_0/jars/hive-shims-scheduler-2.3.9.jar:/opt/spark3_2_0/jars/spark-kubernetes_2.12-3.2.0.jar:/opt/spark3_2_0/jars/hive-shims-0.23-2.3.9.jar:/opt/spark3_2_0/jars/jul-to-slf4j-1.7.30.jar:/opt/spark3_2_0/jars/JLargeArrays-1.5.jar:/opt/spark3_2_0/jars/tink-1.6.0.jar:/opt/spark3_2_0/jars/spark-sql_2.12-3.2.0.jar:/opt/spark3_2_0/jars/transaction-api-1.1.jar:/opt/spark3_2_0/jars/netty-all-4.1.68.Final.jar:/opt/spark3_2_0/jars/jakarta.annotation-api-1.3.5.jar:/opt/spark3_2_0/jars/commons-lang3-3.12.0.jar:/opt/spark3_2_0/jars/hive-cli-2.3.9.jar:/opt/spark3_2_0/jars/zstd-jni-1.5.0-4.jar:/opt/spark3_2_0/jars/scala-collection-compat_2.12-2.1.1.jar:/opt/spark3_2_0/jars/jodd-core-3.5.2.jar:/opt/spark3_2_0/jars/spark-mllib_2.12-3.2.0.jar:/opt/spark3_2_0/jars/commons-net-3.1.jar:/opt/spark3_2_0/jars/univocity-parsers-2.9.1.jar:/opt/spark3_2_0/jars/lapack-2.2.0.jar:/opt/spark3_2_0/jars/snappy-java-1.1.8.4.jar:/opt/spark3_2_0/jars/scala-xml_2.12-1.2.0.jar:/opt/spark3_2_0/jars/zookeeper-3.6.2.jar:/opt/spark3_2_0/jars/arrow-memory-core-2.0.0.jar:/opt/spark3_2_0/jars/okio-1.14.0.jar:/opt/spark3_2_0/jars/rocksdbjni-6.20.3.jar:/opt/spark3_2_0/jars/kubernetes-model-admissionregistration-5.4.1.jar:/opt/spark3_2_0/jars/orc-mapreduce-1.6.11.jar:/opt/spark3_2_0/jars/httpclient-4.5.13.jar:/opt/spark3_2_0/jars/scala-reflect-2.12.15.jar:/opt/spark3_2_0/jars/jta-1.1.jar:/opt/spark3_2_0/jars/json4s-jackson_2.12-3.7.0-M11.jar:/opt/spark3_2_0/jars/orc-core-1.6.11.jar:/opt/spark3_2_0/jars/objenesis-2.6.jar:/opt/spark3_2_0/jars/arpack-2.2.0.jar:/opt/spark3_2_0/jars/kryo-shaded-4.0.2.jar:/opt/spark3_2_0/jars/core-1.1.2.jar:/opt/spark3_2_0/jars/jakarta.servlet-api-4.0.3.jar:/opt/spark3_2_0/jars/jersey-container-servlet-core-2.34.jar:/opt/spark3_2_0/jars/jackson-module-scala_2.12-2.12.3.jar:/opt/spark3_2_0/jars/json4s-ast_2.12-3.7.0-M11.jar:/opt/spark3_2_0/jars/RoaringBitmap-0.9.0.jar:/opt/spark3_2_0/jars/kubernetes-model-node-5.4.1.jar:/opt/spark3_2_0/jars/pyrolite-4.30.jar:/opt/spark3_2_0/jars/metrics-json-4.2.0.jar:/opt/spark3_2_0/jars/jline-2.14.6.jar:/opt/spark3_2_0/jars/hk2-utils-2.6.1.jar:/opt/spark3_2_0/jars/hadoop-shaded-guava-1.1.1.jar:/opt/spark3_2_0/jars/breeze-macros_2.12-1.2.jar:/opt/spark3_2_0/jars/spark-mesos_2.12-3.2.0.jar:/opt/spark3_2_0/jars/curator-framework-2.13.0.jar:/opt/spark3_2_0/jars/bonecp-0.8.0.RELEASE.jar:/opt/spark3_2_0/jars/hive-storage-api-2.7.2.jar:/opt/spark3_2_0/jars/HikariCP-2.5.1.jar:/opt/spark3_2_0/jars/curator-recipes-2.13.0.jar:/opt/spark3_2_0/jars/datanucleus-rdbms-4.1.19.jar:/opt/spark3_2_0/jars/libthrift-0.12.0.jar:/opt/spark3_2_0/jars/zjsonpatch-0.3.0.jar:/opt/spark3_2_0/jars/kubernetes-model-discovery-5.4.1.jar:/opt/spark3_2_0/jars/velocity-1.5.jar:/opt/spark3_2_0/jars/kubernetes-model-policy-5.4.1.jar:/opt/spark3_2_0/jars/kubernetes-model-extensions-5.4.1.jar:/opt/spark3_2_0/jars/hive-shims-common-2.3.9.jar:/opt/spark3_2_0/jars/jaxb-api-2.2.11.jar:/opt/spark3_2_0/jars/jackson-dataformat-yaml-2.12.3.jar:/opt/spark3_2_0/jars/curator-client-2.13.0.jar:/opt/spark3_2_0/jars/spark-kvstore_2.12-3.2.0.jar:/opt/spark3_2_0/jars/json4s-core_2.12-3.7.0-M11.jar:/opt/spark3_2_0/jars/spark-network-common_2.12-3.2.0.jar:/opt/spark3_2_0/jars/kubernetes-model-networking-5.4.1.jar:/opt/spark3_2_0/jars/spark-unsafe_2.12-3.2.0.jar:/opt/spark3_2_0/jars/hive-metastore-2.3.9.jar:/opt/spark3_2_0/jars/javassist-3.25.0-GA.jar:/opt/spark3_2_0/jars/commons-io-2.8.0.jar:/opt/spark3_2_0/jars/parquet-hadoop-1.12.1.jar:/opt/spark3_2_0/jars/arpack_combined_all-0.1.jar:/opt/spark3_2_0/jars/blas-2.2.0.jar:/opt/spark3_2_0/jars/spire_2.12-0.17.0.jar:/opt/spark3_2_0/jars/stream-2.9.6.jar:/opt/spark3_2_0/jars/gson-2.2.4.jar:/opt/spark3_2_0/jars/cats-kernel_2.12-2.1.1.jar:/opt/spark3_2_0/jars/htrace-core4-4.1.0-incubating.jar:/opt/spark3_2_0/jars/arrow-vector-2.0.0.jar:/opt/spark3_2_0/jars/avro-1.10.2.jar:/opt/spark3_2_0/jars/datanucleus-core-4.1.17.jar:/opt/spark3_2_0/jars/commons-cli-1.2.jar:/opt/spark3_2_0/jars/hive-exec-2.3.9-core.jar:/opt/spark3_2_0/jars/breeze_2.12-1.2.jar:/opt/spark3_2_0/jars/avro-mapred-1.10.2.jar:/opt/spark3_2_0/jars/spark-hive-thriftserver_2.12-3.2.0.jar:/opt/spark3_2_0/jars/aircompressor-0.21.jar:/opt/spark3_2_0/jars/metrics-jvm-4.2.0.jar:/opt/spark3_2_0/jars/mesos-1.4.0-shaded-protobuf.jar:/opt/spark3_2_0/jars/automaton-1.11-8.jar:/opt/spark3_2_0/jars/zookeeper-jute-3.6.2.jar:/opt/spark3_2_0/jars/lz4-java-1.7.1.jar:/opt/spark3_2_0/jars/istack-commons-runtime-3.0.8.jar:/opt/spark3_2_0/jars/scala-compiler-2.12.15.jar:/opt/spark3_2_0/jars/paranamer-2.8.jar:/opt/spark3_2_0/jars/kubernetes-model-certificates-5.4.1.jar:/opt/spark3_2_0/jars/ivy-2.5.0.jar:/opt/spark3_2_0/jars/logging-interceptor-3.12.12.jar:/opt/spark3_2_0/jars/commons-dbcp-1.4.jar:/opt/spark3_2_0/jars/kubernetes-model-core-5.4.1.jar:/opt/spark3_2_0/jars/okhttp-3.12.12.jar:/opt/spark3_2_0/jars/shims-0.9.0.jar:/opt/spark3_2_0/jars/oro-2.0.8.jar:/opt/spark3_2_0/jars/parquet-jackson-1.12.1.jar:/opt/spark3_2_0/jars/annotations-17.0.0.jar:/opt/spark3_2_0/jars/hive-shims-2.3.9.jar:/opt/spark3_2_0/jars/jcl-over-slf4j-1.7.30.jar:/opt/spark3_2_0/jars/kubernetes-model-storageclass-5.4.1.jar:/opt/spark3_2_0/jars/commons-collections-3.2.2.jar:/opt/spark3_2_0/jars/commons-compiler-3.0.16.jar:/opt/spark3_2_0/jars/hadoop-client-runtime-3.3.1.jar:/opt/spark3_2_0/jars/hk2-api-2.6.1.jar:/opt/spark3_2_0/jars/aopalliance-repackaged-2.6.1.jar:/opt/spark3_2_0/jars/minlog-1.3.0.jar:/opt/spark3_2_0/jars/osgi-resource-locator-1.0.3.jar:/opt/spark3_2_0/jars/jdo-api-3.0.1.jar:/opt/spark3_2_0/jars/hive-service-rpc-3.1.2.jar:/opt/spark3_2_0/jars/scala-parser-combinators_2.12-1.1.2.jar:/opt/spark3_2_0/jars/commons-pool-1.5.4.jar:/opt/spark3_2_0/jars/kubernetes-model-scheduling-5.4.1.jar:/opt/spark3_2_0/jars/spark-core_2.12-3.2.0.jar:/opt/spark3_2_0/jars/generex-1.0.2.jar:/opt/spark3_2_0/jars/javax.jdo-3.2.0-m3.jar:/opt/spark3_2_0/jars/metrics-core-4.2.0.jar:/opt/spark3_2_0/jars/spark-catalyst_2.12-3.2.0.jar:/opt/spark3_2_0/jars/spark-yarn_2.12-3.2.0.jar:/opt/spark3_2_0/jars/jaxb-runtime-2.3.2.jar:/opt/spark3_2_0/jars/kubernetes-model-autoscaling-5.4.1.jar:/opt/spark3_2_0/jars/spire-platform_2.12-0.17.0.jar:/opt/spark3_2_0/jars/hadoop-client-api-3.3.1.jar:/etc/hadoop/conf/\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:java.library.path=/opt/venv/lib64:/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:java.io.tmpdir=/tmp\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:java.compiler=<NA>\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:os.name=Linux\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:os.arch=amd64\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:os.version=3.10.0-1062.12.1.el7.x86_64\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:user.name=openeo\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:user.home=/home/openeo\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:user.dir=/opt\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:os.memory.free=1131MB\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:os.memory.max=24028MB\\n22/11/02 07:22:47 INFO ZooKeeper: Client environment:os.memory.total=1512MB\\n22/11/02 07:22:47 INFO ZooKeeper: Initiating client connection, connectString=epod-master1.vgt.vito.be:2181,epod-master2.vgt.vito.be:2181,epod-master3.vgt.vito.be:2181 sessionTimeout=50000 watcher=org.apache.accumulo.fate.zookeeper.ZooSession$ZooWatcher@615e83ac\\n22/11/02 07:22:47 INFO X509Util: Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation\\n22/11/02 07:22:47 INFO ClientCnxnSocket: jute.maxbuffer value is 1048575 Bytes\\n22/11/02 07:22:47 INFO ClientCnxn: zookeeper.request.timeout value is 0. feature enabled=false\\n22/11/02 07:22:47 INFO ClientCnxn: Opening socket connection to server epod-master2.vgt.vito.be/192.168.207.57:2181.\\n22/11/02 07:22:47 INFO ClientCnxn: SASL config status: Will not attempt to authenticate using SASL (unknown error)\\n22/11/02 07:22:47 INFO ClientCnxn: Socket connection established, initiating session, client: /192.168.207.55:46202, server: epod-master2.vgt.vito.be/192.168.207.57:2181\\n22/11/02 07:22:47 INFO ClientCnxn: Session establishment complete on server epod-master2.vgt.vito.be/192.168.207.57:2181, session id = 0x2841d4f04f7c698, negotiated timeout = 40000\\nCreated token for: AccumuloDelegationToken-952b4740-0738-46d1-a031-11a0c00a801d , expires: 1667395367772\\n22/11/02 07:22:47 INFO Client: Submitting application application_1666939039227_12581 to ResourceManager\\n22/11/02 07:22:48 INFO YarnClientImpl: Submitted application application_1666939039227_12581\\n22/11/02 07:22:48 INFO Client: Application report for application_1666939039227_12581 (state: ACCEPTED)\\n22/11/02 07:22:48 INFO Client: \\n\\t client token: N/A\\n\\t diagnostics: [Wed Nov 02 08:22:47 +0100 2022] Application is Activated, waiting for resources to be assigned for AM.  Last Node which was processed for the application : epod018.vgt.vito.be:45454 ( Partition : [], Total resource : <memory:118784, vCores:28>, Available resource : <memory:4608, vCores:4> ). Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:17182720, vCores:4800> ; Queue\\'s Absolute capacity = 23.0 % ; Queue\\'s Absolute used capacity = 18.270834 % ; Queue\\'s Absolute max capacity = 100.0 % ; Queue\\'s capacity (absolute resource) = <memory:3952025, vCores:1104> ; Queue\\'s used capacity (absolute resource) = <memory:1661952, vCores:877> ; Queue\\'s max capacity (absolute resource) = <memory:17182720, vCores:4800> ; \\n\\t ApplicationMaster host: N/A\\n\\t ApplicationMaster RPC port: -1\\n\\t queue: default\\n\\t start time: 1667373767810\\n\\t final status: UNDEFINED\\n\\t tracking URL: https://epod-master2.vgt.vito.be:8090/proxy/application_1666939039227_12581/\\n\\t user: openeo\\n22/11/02 07:22:48 INFO ShutdownHookManager: Shutdown hook called\\n22/11/02 07:22:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-d7ebb0ef-d05d-4160-beb6-9c0f3196a6ea\\n22/11/02 07:22:48 INFO ShutdownHookManager: Deleting directory /tmp/spark-d5277ae0-9643-4151-aaf6-654d403f8227\\n'},\n",
       " {'id': 'openeo-yarn-index-1m-2022.10.18-000013/k-Y4N4QBMcjMPc-fsBsh',\n",
       "  'time': '2022-11-02T07:22:51.622Z',\n",
       "  'level': 'info',\n",
       "  'message': 'mapped job_id j-3bf0e39847ee4745803808be3996d20b to application ID application_1666939039227_12581'},\n",
       " {'id': 'openeo-yarn-index-1m-2022.10.18-000013/lvY9N4QBVWXUH_mWjBF5',\n",
       "  'time': '2022-11-02T07:28:08.968Z',\n",
       "  'level': 'info',\n",
       "  'message': 'changed job j-3bf0e39847ee4745803808be3996d20b status from queued to error'},\n",
       " {'id': 'openeo-yarn-index-1m-2022.10.18-000013/l_Y9N4QBVWXUH_mWjBF5',\n",
       "  'time': '2022-11-02T07:28:08.984Z',\n",
       "  'level': 'warning',\n",
       "  'message': 'Could not derive result metadata from /data/projects/OpenEO/j-3bf0e39847ee4745803808be3996d20b/job_metadata.json'},\n",
       " {'id': 'openeo-yarn-index-1m-2022.10.18-000013/mPY9N4QBVWXUH_mWjBF5',\n",
       "  'time': '2022-11-02T07:28:09.092Z',\n",
       "  'level': 'info',\n",
       "  'message': 'marked j-3bf0e39847ee4745803808be3996d20b as done'},\n",
       " {'id': 'error',\n",
       "  'level': 'error',\n",
       "  'message': 'Traceback (most recent call last):\\n  File \"/opt/venv/lib64/python3.8/site-packages/openeogeotrellis/backend.py\", line 1913, in get_log_entries\\n    with (self.get_job_output_dir(job_id) / \"log\").open(\\'r\\') as f:\\n  File \"/usr/lib64/python3.8/pathlib.py\", line 1221, in open\\n    return io.open(self, mode, buffering, encoding, errors, newline,\\n  File \"/usr/lib64/python3.8/pathlib.py\", line 1077, in _opener\\n    return self._accessor.open(self, flags, mode)\\nFileNotFoundError: [Errno 2] No such file or directory: \\'/data/projects/OpenEO/j-3bf0e39847ee4745803808be3996d20b/log\\'\\n'}]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_job.logs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3a4f91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bac3ed2dc488f2c378e3bf5b6869692f562f2ddad183c0dcb5bc297978f34f8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
